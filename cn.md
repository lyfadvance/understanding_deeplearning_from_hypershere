# 从超球面非欧几何视角理解深度学习

**摘要**：本文旨在建立深度学习模型中欧式空间与超球面几何之间的深刻联系。我们证明，通过将数据映射到高维超球面，并利用球极投影将欧式空间中的球形决策边界等价转化为超球面上的线性（超平面）决策边界，可以构建一种具有理论保证的表示学习框架。在此框架下，我们证明了存在一类保球非线性变换，该变换能够对数据进行无损重排，并最终使任意标注的二分类数据集变得线性可分。进一步，我们探讨了在多超平面划分下保持超球面结构的条件，并将其与可逆神经网络的理论相连接。这项工作为理解深度神经网络的表征能力、优化景观以及设计新型架构提供了统一的几何视角。

**关键词**：超球面几何；万能逼近；表示学习；球极投影；可逆神经网络；深度学习理论

## 引言

深度学习的成功常被归因于深度神经网络的强大表示能力。然而，其内部工作机制，尤其是数据表征在训练过程中的演化几何，仍不甚明晰。传统分析多基于欧式空间，本文将视角转向非欧的超球面几何，旨在揭示其与深度学习核心原理的内在关联。

核心假设是，一个理想的表示学习过程应能在不破坏数据整体结构的前提下（即进行“重排列”），逐步重塑特征空间，最终实现线性可分。我们首先在欧式空间中，基于球形划分，构建了一个理论模型以证明此类过程的存在性。然而，球形划分的精确性使其难以学习。为解决此问题，我们引入了**球极投影**，将欧式空间中的球形划分等价转化为超球面上的超平面划分。这一转化是关键的，它将问题从寻找精确的圆心和半径，转变为学习超球面上的一个方向（法向量），后者在高维空间中更为“容易”（见第3章关于Easy Learning的讨论）。

基于此，我们证明存在一种保球的非线性变换，该变换由一系列依赖于数据的条件性正交变换（如Householder反射）构成，能够在保持数据点位于超球面上的同时，逐步调整其相对位置，直至达成线性可分的目标。最后，我们将此框架推广到多超平面划分的情形，并发现其结构与可逆神经网络（Invertible Neural Networks）中的耦合层设计不谋而合，从而为这类模型的几何解释提供了新的见解。

### **相关工作**

在几何深度学习的理论图景中，超球面流形（Hyperspherical Manifold）因其独特的对称性与紧致性，已成为理解与设计神经网络表示的重要基石。现有研究主要从两个相互关联但各有侧重的视角展开：一是将超球面视为**数据表征的自然空间**，以此设计损失函数与优化目标，以提升模型的判别性能；二是将其视为**对称性作用的载体**，以此构建具有特定等变性的网络架构，以嵌入物理世界的归纳偏置。本章节将梳理这些工作，并以此定位本研究的理论出发点。

#### **1. 超球面表征与判别性学习**

一系列富有影响力的工作表明，将特征约束或映射到超球面上，能够显著增强深度学习模型的判别能力，尤其是在度量学习任务中。这类研究的核心在于利用超球面上的**角度距离**（Angular Distance）或**测地线距离**（Geodesic Distance）来替代传统的欧氏距离，从而获得更具判别性和语义一致性的特征空间。例如，在面部识别领域，一系列基于角度间隔（Angular  Margin）的损失函数（如SphereFace、CosFace、ArcFace）被提出，通过直接在超球面流形上施加约束，使得类内特征更紧凑、类间特征分离更明显，取得了突破性成果。

近期研究进一步探索了**数据空间到超球面的映射过程**本身对学习的影响。《Improvising the Learning of Neural Networks on Hyperspherical Manifold》 一文是这一方向的典型代表。该工作系统地将**球极投影**（Stereographic Projection）作为一种前置变换模块，应用于标准的角间隔损失函数。其实验表明，将数据从欧氏空间显式地映射到超球面流形，能够作为一种有效的归纳偏置，提升模型在CIFAR等图像分类数据集上的性能。这项工作揭示了**流形变换**对于优化过程具有积极的引导作用，但其主要贡献在于工程性能的实证提升，并未深入探讨此种变换所蕴含的、关于表示学习能力极限的基础理论问题。

#### **2. 超球面上的对称性与等变架构**

另一条重要的研究脉络源于几何深度学习的基本思想，即将数据的内在对称性（如平移、旋转、反射）编码至网络结构本身，从而构建**等变**（Equivariant）或**不变**（Invariant）的模型。这一思想在《Geometric deep learning: Grids, groups, graphs, geodesics, and gauges》 等综述性工作中得到了系统阐述。在此框架下，超球面因其自身丰富的对称群（n维正交群O(n)）而成为构建等变网络的理想对象。

代表性工作《On Learning Deep O(n)-Equivariant Hyperspheres》 致力于构建严格对O(n)群变换（即n维旋转与反射）等变的深度学习层。该工作利用球形神经元与规则单纯形等几何工具，设计出网络组件，确保其输出能随输入的正交变换而协同、可预测地变化。此类方法在点云分析、分子建模等**对称性先验至关重要**的领域展现出强大优势，其核心贡献在于提供了一种结构性、可扩展的归纳偏置解决方案。

此外，旨在保持信息流的**可逆神经网络**（如RevNet）也从另一个角度提供了洞见。通过设计双射（Bijective）的网络层，这类模型在理论上保证了前向传播中信息的无损性，这与在流形上保持度量或体积的变换思想存在内在的哲学关联。

#### **3. 欧式空间中的局部平移变换**

resnet的残差方法在本文视角内，可以视为在欧式空间中做局部平移变换。本文通过householder反射将其扩展为超球面空间中的局部平移，反射，旋转变换。

#### **4. 本文研究定位**

前述工作分别从**判别性能的工程优化**和**对称性的结构嵌入**两个维度，推动了超球面深度学习的发展。然而，一个更为基础的理论问题在于：抛开具体的任务目标或预设的对称性，表示空间本身通过一系列非线性变换，其**形态塑造的能力极限**究竟何在？

本文的研究视角正源于此。与优化特定损失函数或预设对称性约束的路径不同，本文旨在构建一个探究表示学习**理论可达性**的框架。我们证明，通过球极投影将数据映射至超球面后，存在一系列由超平面划分引导的、**保球的（即维持流形结构）条件性正交变换**，能够实现数据表示的无损重排列，并最终必然导向一个线性可分的表示空间。本框架的核心在于“**变换**”与“**可达性**”，它并不旨在提出一个替代性的实用损失函数或网络层，而是为理解深度网络中复杂的层叠操作如何协同工作以塑造表示空间，提供了一个抽象但具有完备性保证的理论透镜。这一基础性探讨，亦能为前述应用导向的研究中观察到的性能提升现象，提供更深层的理论注脚。

**核方法与特征空间**：我们的球极投影 $\Psi$ 与核方法中的特征映射有相似之处，它将数据隐式映射到高维（甚至无限维）空间以线性化问题[6]。本文明确了这种映射的几何目标是获得一个超球面表示。

**表示学习的度量观点**：将学习过程视为在表示空间中进行“重排列”或“度量重塑”的观点与一些表示学习理论相契合[8]。本文的超球面视角为这种重塑过程提供了一个易于理论分析的几何舞台。

## 基于重排列的万能逼近定理

### 欧式空间中的球形划分

考虑一个有限数据集 $X = \{ \mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_n \}$，其中 $\mathbf{x}_i \in \mathbb{R}^D$，及其对应的二分类标签 $B = (b_1, b_2, ..., b_n), b_i \in \{0, 1\}$。我们的目标是构造一个模型 $f(\cdot; W)$，其参数为 $W$，满足以下性质：

1.  **保排列性**：对任意参数 $W$，输出 $X' = f(X; W)$ 是输入 $X$ 的一个排列（重排列）。
2.  **排列完备性**：对 $X$ 的任意排列 $X'_{\text{perm}}$，存在参数 $W$ 使得 $f(X; W) = X'_{\text{perm}}$。
3.  **线性可分性**：存在参数 $W^*$，使得重排列后的数据集 $X'^* = f(X; W^*)$ 关于标签 $B$ 是线性可分的。

我们首先在允许使用球形符号函数 $\operatorname{sphereSign}(\mathbf{x}; \mathbf{a}, b) = \operatorname{sign}(\|\mathbf{x} - \mathbf{a}\|^2 - b)$、线性函数、条件判断 `IF` 和最近邻选择 `MIN` 操作的框架下进行构造。

**证明概要**：

- **构造固定权重空间**：对于数据集中的每个点 $\mathbf{x}_k$，我们可以构造一个球形划分使其唯一识别该点。一个简单的构造是令圆心 $\mathbf{a}^k = \mathbf{x}^k$，半径 $b = \epsilon$（一个极小的正数）。所有这样的 $(\mathbf{a}^k, b)$ 构成固定权重空间 $S_{\text{fix}} = X$。
- **实现任意两元素交换**：模型 $f$ 的每一步旨在交换数据集中的一对点 $(\mathbf{x}^p, \mathbf{x}^q)$。可学习参数 $w_p, w_q \in S_{\text{learn}}$ 通过 `MIN` 操作从 $S_{\text{fix}}$ 中选择出对应的 $(\mathbf{a}^p, b^p)$ 和 $(\mathbf{a}^q, b^q)$。然后，通过 `IF` 和 `sphereSign` 判断，将位于 $\mathbf{a}^q$ 球内的点（即 $\mathbf{x}^q$）移动到 $\mathbf{a}^p$，反之亦然，完成交换。通过组合多个这样的交换步骤，可以实现任意排列。
- **存在线性可分排列的证明**：对于 $\mathbb{R}^D$ 中任意 $n$ 个互异的点，以及任意整数 $p$ ($0 \le p \le n$)，总存在一个超平面恰好将点集划分为含有 $p$ 个点和 $n-p$ 个点的两个部分[1]。由于我们的标签 $B$ 将数据划分为两个集合（大小分别为 $p$ 和 $n-p$），因此总能找到一个超平面将这两个集合分开。进而，通过上述交换操作，总可以学习到一个排列 $W^*$，将标签相同的点移动到超平面的同一侧，从而实现线性可分。

下面给出详细证明过程。

假设权重由固定权重参数$W_{ul} = X$，以及可学习权重参数$W_{l}$组成。这种方式加上$MIN_x$操作可以允许$W_l$在全空间中学习。

显而易见对于数据集中的任意$x^k \in X$，存在$a^k, b^k$，使得
$$
\begin{align}
sphereSign(x_m, a^k, b^k) > 0 \ \ \ \ \ \ \ \ \   k = m \\
 sphereSign(x_m, a^kk, b^k) < 0 \ \ \ \ \ \ \ \ \   k != m \\
\end{align}
$$
对所有的$x^k$，我们构造出这样$a^k, b^k$。所有的$a^k , b^k$构成固定权重空间$SPACE_{fix} = \{(a^k, b^k) | k = 1...n\}$。可以直接假设$a^k = x^k, b= \epsilon$，因此这个固定权重空间其实就是数据集X本身。

下面，我们构造模型结构$f$为，每一步对数据集中的两个数据进行交换。因此每一步的可学习权重为$w_p$和$w_q$。并且$w_p, w_q \in SPACE_W$。每一步的所有可学习权重为$SPACE_{learn}$。

假设当前步所需交换的数据为$x^p, x^q$，则具体如下:

> $a^q, b^q = MIN(w_q, SPACE_{fix})$
>
> $a^p, b^p = MIN(w_p, SPACE_{fix})$
>
> $IF$ $sphereSign(x, a^q, b^q)$:
>
> ​	$x = a^p$
>
> $ELSE\ IF $ $sphereSign(x, a^p, b^p)$:
>
> $x = a^q$
>
> $ELSE$:
>
> $x = x$



通过如上操作完成数据集的重排列，并且上面的过程可以保证

1. 对任意的$W$, 得到$X' = f(X,W)$；其中$X'$必为$X$的重排列。
2. 对任意的$X'$，$X'$为$X$的重排列，必存在$W$，使得$f(X,W)=X'$；



下面证明如下定理:

**定理**：设$ S={x_1,x_2,…,x_m} $是 $R_n$ 中 m个互不相同的点。则对于任意整数 $p(0≤p≤m)$，总存在一个超平面 H 将空间划分为两个开半空间，使得其中一个半空间恰好包含 $p$个点，另一个包含 $m−p$ 个点，且没有任何点落在超平面H上。

**步骤一：存在一个“好”的方向**
我们需要找到一个单位向量 $u∈R_n$，使得对于任意两个不同的点 $x_i≠x_j$，都有 $u⋅x_i≠u⋅x_j$。

- **理由**：对于任意一对固定的 $(i,j)$，满足 $u⋅(x_i − x_j)=0$的 $u$ 的集合，是 n 维空间中一个过原点的超平面（即所有与向量$ (x_i−x_j)$ 垂直的向量构成的集合）。其与单位球面的交集是一个“大圆”（或低维球面），**测度为零**。
- 由于只有有限对 $(i,j)$（共 $C_m^2$ 对），所有“坏方向”（即使任意两点投影相同的方向）的集合是有限个零测集的并集，**仍然是一个零测集**。
- 因此，在单位球面上，**几乎所有**方向都是“好方向”。我们只要任意选取一个即可（例如，可以随机生成一个向量，几乎必然满足条件）。

**步骤二：投影与排序**
固定一个这样的“好方向” $u$。计算每个点在该方向上的投影（标量）：
$$
y_i=u⋅x_i,i=1,2,…,m.
$$


由步骤一可知，所有 $y_i$ 两两不同。将它们按严格递增顺序排列：
$$
y(1)<y(2)<⋯<y(m).
$$


这里 $y(k)$ 对应某个点 $x(k)$，这个顺序是点集沿$u$方向的**一个全序**。

**步骤三：构造超平面**
现在，对于给定的整数 $p(0≤p≤m_0≤p≤m)$：

1. 当 $1≤p≤m−1$ 时，选取一个实数 t，使得 $y(p)<t<y(p+1)$。
2. 当 $p=0$时，选取$ t<y(1)$。
3. 当 $p=m$时，选取 $t>y(m)$。

考虑由方程 $u⋅x=t$ 所定义的超平面 H。

**步骤四：验证划分**

- **无点落在平面上**：对于任意点 $x_i$，其投影 $y_i$要么小于$t$，要么大于 $t$，因为 $t$不等于任何一个 $y_i$。所以 $u⋅xi≠t$，即没有点在 H 上。
- **精确计数**：
  - 对于满足 $u⋅x_i<t$的点，其投影 $y_i$ 小于 t。根据 $t$ 的选取方式，**恰好有 p 个这样的点**（即投影为$y(1),y(2),…,y(p)$的点）。
  - 剩下的 $m−p$ 个点必然满足 $u⋅x_i>t$，位于超平面的另一侧。

至此，超平面 H 严格地将点集划分为含有 $p$个点和 $m−p$ 个点的两个部分，完成了证明。

至此，我们证明了在欧式空间中，基于球形划分的模型可以满足上述三个性质。然而，这种构造依赖于对数据点的精确记忆和匹配，且球形划分的参数（圆心和半径）在高维空间中难以通过随机初始化后的梯度下降有效学习。

### 向超球面几何的转化：球极投影

为解决球形划分的“难学习”问题，我们引入球极投影，将问题转化到超球面几何中。核心洞察是：**$n$ 维欧式空间中的球形决策边界，可以等价地转化为 $n$ 维超球面上的超平面决策边界。**

**构造**：定义映射 $\Phi: \mathbb{R}^n \rightarrow \mathbb{R}^{n+2}$，
$$
\Phi(\mathbf{x}) = (1, \mathbf{x}, \|\mathbf{x}\|^2).
$$
归一化后得到球极投影 $\Psi: \mathbb{R}^n \rightarrow S^{n+1} \subset \mathbb{R}^{n+2}$，
$$
\Psi(\mathbf{x}) = \frac{\Phi(\mathbf{x})}{\|\Phi(\mathbf{x})\|}.
$$

**等价性**：考虑欧式空间中的球形划分 $\operatorname{sign}(\|\mathbf{x} - \mathbf{a}\|^2 - b)$。将其展开：
$$
\|\mathbf{x} - \mathbf{a}\|^2 - b = \|\mathbf{x}\|^2 - 2\mathbf{a}\cdot\mathbf{x} + \|\mathbf{a}\|^2 - b.
$$
定义向量 $\mathbf{w} = (\|\mathbf{a}\|^2 - b, -2a_1, ..., -2a_n, 1) \in \mathbb{R}^{n+2}$。则有：
$$
\mathbf{w} \cdot \Phi(\mathbf{x}) = \|\mathbf{x} - \mathbf{a}\|^2 - b.
$$
由于 $\Phi(\mathbf{x}) = \|\Phi(\mathbf{x})\| \Psi(\mathbf{x})$，且 $\|\Phi(\mathbf{x})\| > 0$，可得：
$$
\operatorname{sign}(\|\mathbf{x} - \mathbf{a}\|^2 - b) = \operatorname{sign}(\mathbf{w} \cdot \Psi(\mathbf{x})).
$$
因此，欧式空间中的球形划分 $\operatorname{sign}(\|\mathbf{x} - \mathbf{a}\|^2 - b)$ 等价于超球面 $S^{n+1}$ 上的超平面划分 $\operatorname{sign}(\mathbf{w} \cdot \Psi(\mathbf{x}))$。

这一转化意义重大。它意味着，在超球面表示下，我们只需使用**简单的线性判别函数** $\operatorname{sign}(\mathbf{w} \cdot \mathbf{z})$（其中 $\mathbf{z} \in S^{n+1}$），就可以实现原欧式空间中复杂球形划分的表达能力。结合2.1节的论证，我们得到以下定理：

**定理 2.1 (超球面重排列与线性可分)**：存在一个模型 $f$，该模型首先通过球极投影 $\Psi$ 将数据 $X \subset \mathbb{R}^D$ 映射到超球面 $S^{D+1}$ 上，随后仅允许使用线性判别函数、条件操作和最近邻选择，使得：

      1. 对任意参数 $W$，输出为输入数据在超球面上的一个重排列。
      2. 对任意重排列，存在参数 $W$ 实现之。
      3. 存在参数 $W^*$，使得重排列后的数据表示线性可分。

## Easy Learning 的几何视角

第2.1节指出，直接在欧式空间学习精确的球形划分是困难的。我们形式化“容易学习”(Easy Learning)的概念：假设模型参数 $W$ 以 $N(0, \sigma^2 I)$ 初始化，其分布在高维空间中近似于一个超球面 $S_{\text{param}}^m$。如果对于 $S_{\text{param}}^m$ 上几乎任意一点（即随机初始化的 $W$），都存在一个小的扰动 $\Delta W$，使得 $W + \Delta W$ 是某个问题解的局部极小值，那么该问题是易于学习的。

球形划分之所以难，是因为其解（特定的 $(\mathbf{a}, b)$）在高维参数空间中像是“孤岛”，随机初始化的 $W$ 落在解附近的概率极低。相反，超平面划分 $\operatorname{sign}(\mathbf{w} \cdot \mathbf{z})$ 的解（法向量 $\mathbf{w}$）的“方向”是关键的，而其模长不影响符号函数。在高维超球面上，几乎所有的方向都可能是“好”的初始方向（见第2.1节超平面存在性证明中对“好方向”的讨论），且通过旋转（梯度更新）更容易到达目标方向。因此，在超球面表示下使用超平面划分，更符合“Easy Learning”的直觉。

## 超球面上的保球非线性变换

定理2.1的构造需要记忆整个数据集，不具备泛化能力。我们转向一个更现实的设定：假设数据均匀分布（或来自）一个超球面 $S^{m-1}$。我们探究是否存在一个参数化的非线性变换 $T: S^{m-1} \rightarrow S^{m-1}$，它不仅能保持数据位于球面上（保球），还能通过调整参数，使变换后的数据对任意给定的二分类标签线性可分。

### 单超平面划分下的变换

一个自然的构造是迭代地进行以下两步操作：

1.  **划分**：用一个超平面 $H: \mathbf{a} \cdot \mathbf{z} = 0$（穿过球心）将超球面 $S^{m-1}$ 划分为两个半球。
2.  **条件旋转**：选择其中一个半球（一个对称的几何体），并绕该半球的对称中心（即球心）施加一个旋转。这可以通过Householder反射实现。householder反射在超球面几何中占据异常重要的地位。通过连续householder反射可以完成超球面几何中某个区域的所有平移，旋转，反射操作。因此我们用householder反射实现对称几何体围绕该对称中心旋转的操作。给定两个与 $\mathbf{a}$ 正交的单位向量 $\mathbf{u}', \mathbf{v}'$，其生成的旋转可以表示为两个Householder反射的复合 $H_{\mathbf{u}'} H_{\mathbf{v}'}$。此操作仅作用于所选的半球。

该变换 $T$ 显然是保球的。感性上，通过一系列这样的“切割-旋转”操作，我们可以像搅拌咖啡中的奶泡一样，将属于不同类别的数据点逐渐分离到球面上不同的区域，直至存在一个超平面能将其分开。严格的证明需要更复杂的几何论证。连续情况证明比较复杂，因此下面我们假设数据集中元素数量是有限的，给出离散情况下的证明，证明如下：

#### 定理 1（极小步长单点移动）

第一步，证明 数据集的点分布在超球面上，对任意超球面上的某个数据点$p$，存在某个测地圆，以该点为圆心，只包含该点。且以该测地圆中的任一点为法向量且经过该点的超平面截出的超球冠中也只包含该点。

设数据集 $$X$$ 位于超球面 $$S^{d-1} \subset \mathbb{R}^d$$ 上，且 $$X$$ 是离散的（任意两点间的测地距离有正下界）。对于任意一点 $$p \in X$$，定义
$$
\delta = \min\{ d(p, x) : x \in X, x \neq p \} > 0.
$$
取 $$r$$ 满足 $$0 < r < \delta/2$$。考虑以 $$p$$ 为中心、半径为 $$r$$ 的测地盘（geodesic disk）：
$$
B(p, r) = \{ x \in S^{d-1} : d(p, x) < r \}.
$$
由于 $$r < \delta$$，有 $$B(p, r) \cap X = \{p\}$$。

现在，考虑测地圆（geodesic circle）：
$$
C(p, r) = \{ q \in S^{d-1} : d(p, q) = r \}.
$$
对于任意 $$q \in C(p, r)$$，构造超平面：
$$
H_q = \{ x \in \mathbb{R}^d : (x - p) \cdot q = 0 \},
$$
它以 $$q$$ 为法向量且经过点 $$p$$。这个超平面将超球面分成两个闭超球冠：
$$
C_q^+ = \{ x \in S^{d-1} : (x - p) \cdot q \geq 0 \}, \quad C_q^- = \{ x \in S^{d-1} : (x - p) \cdot q \leq 0 \}.
$$
下面证明 $$C_q^+ \cap X = \{p\}$$。

设 $$x \in X \setminus \{p\}$$，令 $$\theta = d(p, x) \geq \delta > 2r$$。将点视为单位向量，则 $$p \cdot q = \cos r$$。由球面三角不等式，$$d(q, x) \in [\theta - r, \theta + r]$$，因此
$$
\cos(\theta + r) \leq x \cdot q \leq \cos(\theta - r).
$$
由于 $$\theta > 2r$$，有 $$\theta - r > r$$，且余弦函数在 $$[0, \pi]$$ 上单调递减，所以
$$
x \cdot q \leq \cos(\theta - r) < \cos r = p \cdot q.
$$
因此，$$(x - p) \cdot q < 0$$，即 $$x \notin C_q^+$$。故 $$C_q^+ \cap X = \{p\}$$。

综上所述，对于任意点 $$p \in X$$，存在半径 $$r > 0$$，使得以 $$p$$ 为中心的测地盘 $$B(p, r)$$ 只包含 $$p$$，并且对于该测地圆 $$C(p, r)$$ 上的任意点 $$q$$，以 $$q$$ 为法向量且经过 $$p$$ 的超平面截出的超球冠 $$C_q^+$$ 也只包含 $$p$$。证毕。

第二步。设常数 $\varepsilon_0$  为第一步中所取测地圆的半径，即 $\varepsilon_0 = r$。则对于任意点 $q \in S^{d-1}$ 满足 $0 <  \text{dist}(p,q) \leq \varepsilon_0$，存在点 $r \in C(p,\varepsilon_0)$  和一个旋转 $R$（旋转轴在测地圆中，旋转角度为 $\text{dist}(p,q)$），使得 $R(p) =  q$。

构建p到q的测地线，以测地线的中心构建w，测地线长度的一半构建b，然后以W为旋转中心，额外选一个旋转平面，旋转180度，将P旋转到Q。

#### 定理 2（有限步线性可分）

对任意有限标签配置 $f:X\to\{0,1\}$，存在有限步上述 $\varepsilon_0$-移动，使得最终所有 $1$ 点位于公共半球内（即线性可分）。

证明

- 以 $\varepsilon_0$ 为步长，从任意 $p\in X$ 到任意目标 $q\in\mathbb{S}^{d-1}$ 的最短测地长度 $\leq\pi$；
- 所需步数上限

$$
N_{\max}=\left\lceil\frac{\pi}{\varepsilon_0}\right\rceil
$$

（与 $X$ 有关，与 $p,q$ 无关）。

- 对所有“南半球 $1$ 点”逐个执行 $\varepsilon_0$-步长链式移动，每次把它推到北极附近；
- 每点至多 $N_{\max}$ 步；
- 总步数 $\leq|X|\cdot N_{\max}$ 有限；
- 最终所有 $1$ 点位于公共半球，线性可分。

### 多超平面划分与可逆结构

单步仅使用一个超平面效率低下。我们希望每一步能用多个超平面进行更精细的划分。然而，对由多个超平面划分出的任意区域直接进行旋转，通常无法保证整体变换是保球的。

**关键问题**：在什么条件下，多超平面划分下的条件变换能保持超球面？

**答案源于可逆神经网络**[2, 3]。考虑将空间分解为 $\mathbb{R}^m = \mathbb{R}^{m-k} \times \mathbb{R}^k$，点记为 $\mathbf{z} = (\mathbf{x}, \mathbf{y})$。我们限制划分超平面的法向量仅依赖于前 $m-k$ 个坐标，即具有形式 $\mathbf{a}^{(j)} = (\mathbf{a}_0^{(j)}, \mathbf{0})$。那么，划分仅由 $\mathbf{x}$ 决定，产生区域 $\{A_\ell\}$。在超球面 $S^{m-1}$ 上，对应区域为：
$$
\tilde{R}_\ell = \{ (\mathbf{x}, \mathbf{y}) \in S^{m-1} | \mathbf{x} \in A_\ell \}.
$$
对于固定的 $\mathbf{x}$，点 $(\mathbf{x}, \mathbf{y})$ 中的 $\mathbf{y}$ 部分位于一个 $k-1$ 维球面 $S^{k-1}(\sqrt{1-\|\mathbf{x}\|^2})$ 上。这种结构称为**局部乘积结构**。

在这种结构下，我们可以定义保球变换：
$$
T(\mathbf{x}, \mathbf{y}) = (\mathbf{x}, O_\ell(\mathbf{x}) \mathbf{y}), \quad \text{若 } (\mathbf{x}, \mathbf{y}) \in \tilde{R}_\ell.
$$
其中 $O_\ell(\mathbf{x}) \in O(k)$ 是一个依赖于 $\mathbf{x}$（从而依赖于区域 $\ell$）的正交矩阵。由于正交变换保持 $\mathbf{y}$ 的范数，因此 $\|T(\mathbf{x}, \mathbf{y})\| = \|\mathbf{z}\| = 1$，变换 $T$ 是保球的。特别地，若 $O_\ell(\mathbf{x})$ 由一个Householder反射 $H_{\mathbf{u}(\mathbf{x})}$ 生成，且反射向量 $\mathbf{u}$ 的形式为 $\mathbf{u} = (0, \mathbf{u}_1)$，则变换可具体写为：
$$
\begin{aligned}
\mathbf{x}' &= \mathbf{x}, \\
\mathbf{y}' &= H_{f(\mathbf{x})} \mathbf{y}, \quad \text{其中 } f(\mathbf{x}) = \operatorname{sign}(\mathbf{a}_0 \cdot \mathbf{x} + b) \mathbf{u}_1.
\end{aligned}
$$
这正是**可逆神经网络中耦合层**（如RealNVP [4], GLOW [5]）的核心思想：一部分维度（$\mathbf{x}$）保持不变，并用于生成控制另一部分维度（$\mathbf{y}$）进行变换的参数（此处是反射向量）。我们的分析为其提供了一个清晰的几何解释：**可逆耦合层本质是在具有局部乘积结构的区域上，施加保球的、条件性的正交变换。**

下面详细解释这一过程。


考虑空间 $\mathbb{R}^{m} = \mathbb{R}^{m-k} \times \mathbb{R}^{k}$，点记为 $z = (x,y)$，其中 $x \in \mathbb{R}^{m-k}, y \in \mathbb{R}^{k}$。单位超球面为：
$$
S^{m-1} = \{(x,y) \in \mathbb{R}^{m} : \|x\|^{2} + \|y\|^{2} = 1\}.
$$
设有一组超平面，每个由法向量 $a^{(j)} \in \mathbb{R}^{m}$ 和偏置 $b_{j} \in \mathbb{R}$ 定义，方程为：
$$
a^{(j)} \cdot z = b_{j}, \quad j=1,\ldots,N.
$$
关键限制：所有法向量满足 $a^{(j)} = (a^{(j)}_{0},0)$，其中 $a^{(j)}_{0} \in \mathbb{R}^{m-k}$。即超平面方程仅涉及前 $m-k$ 个坐标：
$$
a^{(j)}_{0} \cdot x = b_{j}.
$$

这些超平面将 $\mathbb{R}^{m-k}$ 划分为若干凸多面体区域 $\{A_{\ell}\}_{\ell \in L}$，每个 $A_{\ell}$ 是 $x$ 空间中的一个开集，由一组严格线性不等式定义。在 $\mathbb{R}^{m}$ 中，对应区域为：
$$
R_{\ell} = \{(x,y) \in \mathbb{R}^{m} : x \in A_{\ell}\}.
$$
在超球面上，区域为：
$$
\tilde{R}_{\ell} = R_{\ell} \cap S^{m-1} = \{(x,y) \in S^{m-1} : x \in A_{\ell}\}.
$$
固定 $x \in A_{\ell}$ 且满足 $\|x\| < 1$（若 $\|x\| = 1$，则 $y = 0$，为孤立点），$y$ 必须满足 $\|y\|^{2} = 1 - \|x\|^{2}$，即 $y$ 属于半径为 $\sqrt{1 - \|x\|^{2}}$ 的球面 $S^{k-1}(\sqrt{1 - \|x\|^{2}})$。因此：
$$
\tilde{R}_{\ell} = \bigcup_{x \in A_{\ell} \cap B_{m-k}} \{x\} \times S^{k-1}\left(\sqrt{1 - \|x\|^{2}}\right),
$$
其中 $B_{m-k} = \{x \in \mathbb{R}^{m-k} : \|x\| \leq 1\}$。每个 $\tilde{R}_{\ell}$ 具有 局部乘积结构：$x$ 部分属于 $A_{\ell}$，$y$ 部分为球面（维数随 $x$ 变化）。


对每个区域 $\tilde{R}_\ell$，定义变换 $T_\ell: \tilde{R}_\ell \rightarrow S^{m-1}$ 为：
$$
T_\ell(x, y) = (x, O_\ell(x)y),
$$
其中 $O_\ell(x) \in O(k)$ 是依赖于 $x$ 的正交矩阵（例如由 Householder 反射生成）。由于 $O_\ell(x)$ 正交，有：
$$
\|O_\ell(x)y\| = \|y\|,
$$
因此：
$$
\|x\|^2 + \|O_\ell(x)y\|^2 = \|x\|^2 + \|y\|^2 = 1,
$$
即 $T_\ell(x, y) \in S^{m-1}$。整体变换 $T: S^{m-1} \rightarrow S^{m-1}$ 由所有 $T_\ell$ 拼接而成：
$$
T(x, y) = T_\ell(x, y) \quad \text{当} \quad (x, y) \in \tilde{R}_\ell.
$$
保持超球面

- 良定性：由于区域 $\{\tilde{R}_\ell\}$ 是 $S^{m-1}$ 的划分（边界测度为零，可忽略），$T$ 在几乎处处有定义。
- 值域在球面上：如上所述，每个 $T_\ell$ 将 $\tilde{R}_\ell$ 映射到 $S^{m-1}$ 的子集，故 $T$ 的像包含于 $S^{m-1}$。
- 满射性：对任意 $(x', y') \in S^{m-1}$，存在 $\ell$ 使得 $x' \in A_\ell$，令 $y = O_\ell(x')^{-1}y'$，则 $(x', y) \in \tilde{R}_\ell$ 且 $T(x', y) = (x', y')$，故 $T$ 是满射。
- 单射性：若 $T(x_1, y_1) = T(x_2, y_2)$，则 $x_1 = x_2$（因 $x$ 部分不变），且 $O_\ell(x_1)y_1 = O_\ell(x_1)y_2$，由正交矩阵可逆得 $y_1 = y_2$。
  因此，$T$ 是 $S^{m-1}$ 到自身的双射，且保持超球面。
  可逆性与几何解释
  由于 $T$ 在每点可逆且逆变换易求（$T^{-1}(x, y) = (x, O_\ell(x)^{-1}y)$），这构成了可逆神经网络的基础。几何上，超平面集合 $\{(a_i, 0)\}$ 在超球面上划分出的每个区域具有 $S^k$ 对称性：对固定 $x$，$y$ 可自由旋转而不改变区域归属。因此，对 $y$ 施加任意正交变换（依赖 $x$）不会破坏超球面，且区域整体保持不变。

## 6. 结论和未来工作

本文系统阐述了超球面几何在理解深度学习中的重要作用。我们证明了通过球极投影，欧式空间中的复杂划分问题可转化为超球面上的线性问题，这不仅具有理论完备性，也更符合梯度下降优化的特性（Easy Learning）。进一步，我们构造了超球面上的保球非线性变换，并论证了其通过迭代的“划分-条件旋转”使数据线性可分的能力。最后，我们将多超平面下的保球变换与可逆神经网络的耦合层联系起来，为后者提供了深刻的几何解释。未来工作包括将此框架扩展到多层深度架构、多分类问题，以及探索更一般的流形上的类比，并设计基于此理论的新型高效、可解释的神经网络模块。

但是，还有一些工作目前没有完成，需要留待以后进一步研究。

第一个是，给出连续情况下的任意可分性证明。

第二，给出基于householder反射的可逆模块的拟合能力证明。

第三， 在超球面上寻找更为广泛的多超平面分割的无损信息模型。

第四，除了超球面模型，是否还有其它流形结构，也存在无损保流形非线性变换。

## 参考文献

[1] Cover, T. M. (1965). Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. *IEEE transactions on electronic computers*, (3), 326-334.

[2] Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., & Jacobsen, J. H. (2019). Invertible residual networks. In *International Conference on Machine Learning* (pp. 573-582). PMLR.

[3] Kobyzev, I., Prince, S. J., & Brubaker, M. A. (2020). Normalizing flows: An introduction and review of current methods. *IEEE transactions on pattern analysis and machine intelligence*, 43(11), 3964-3979.

[4] Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real nvp. *International Conference on Learning Representations*.

[5] Kingma, D. P., & Dhariwal, P. (2018). Glow: Generative flow with invertible 1x1 convolutions. *Advances in neural information processing systems*, 31.

[6] Schölkopf, B., & Smola, A. J. (2002). *Learning with kernels: support vector machines, regularization, optimization, and beyond*. MIT press.

[7] Bronstein, M. M., Bruna, J., Cohen, T., & Veličković, P. (2021). Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. *arXiv preprint arXiv:2104.13478*.

[8] Arora, S., et al. (2019). Theory of deep learning. *Theoretical Foundations of Deep Learning*.



[Improvising the Learning of Neural Networks on Hyperspherical Manifold](https://zhuanzhi.ai/paper/9dca40de43029f58080fbc17ad7c29c4)

[On Learning Deep O(n)-Equivariant Hyperspheres](https://arxiv.org/abs/2305.15613)
